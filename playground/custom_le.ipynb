{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch-geometric\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "from torch_geometric.utils import (\n",
    "    get_laplacian,\n",
    "    to_scipy_sparse_matrix\n",
    ")\n",
    "from typing import Any, Optional\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddCustomLaplacianEigenvectorPE(BaseTransform):\n",
    "    r\"\"\"Adds the Laplacian eigenvector positional encoding from the\n",
    "    `\"Benchmarking Graph Neural Networks\" <https://arxiv.org/abs/2003.00982>`_\n",
    "    paper to the given graph\n",
    "    (functional name: :obj:`add_laplacian_eigenvector_pe`).\n",
    "\n",
    "    Args:\n",
    "        k (int): The number of non-trivial eigenvectors to consider.\n",
    "        attr_name (str, optional): The attribute name of the data object to add\n",
    "            positional encodings to. If set to :obj:`None`, will be\n",
    "            concatenated to :obj:`data.x`.\n",
    "            (default: :obj:`\"laplacian_eigenvector_pe\"`)\n",
    "        is_undirected (bool, optional): If set to :obj:`True`, this transform\n",
    "            expects undirected graphs as input, and can hence speed up the\n",
    "            computation of eigenvectors. (default: :obj:`False`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :meth:`scipy.sparse.linalg.eigs` (when :attr:`is_undirected` is\n",
    "            :obj:`False`) or :meth:`scipy.sparse.linalg.eigsh` (when\n",
    "            :attr:`is_undirected` is :obj:`True`).\n",
    "    \"\"\"\n",
    "    # Number of nodes from which to use sparse eigenvector computation:\n",
    "    SPARSE_THRESHOLD: int = 100\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        k: int,\n",
    "        attr_name: Optional[str] = 'laplacian_eigenvector_pe',\n",
    "        is_undirected: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        self.k = k\n",
    "        self.attr_name = attr_name\n",
    "        self.is_undirected = is_undirected\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def forward(self, data: Data) -> Data:\n",
    "        assert data.edge_index is not None\n",
    "        num_nodes = data.num_nodes\n",
    "        assert num_nodes is not None\n",
    "    \n",
    "        edge_index, edge_weight = get_laplacian(\n",
    "            data.edge_index,\n",
    "            data.edge_weight,\n",
    "            normalization=None,  # Skip Laplacian normalization\n",
    "            num_nodes=num_nodes,\n",
    "        )\n",
    "    \n",
    "        L = to_scipy_sparse_matrix(edge_index, edge_weight, num_nodes)\n",
    "    \n",
    "        if num_nodes < self.SPARSE_THRESHOLD:\n",
    "            from numpy.linalg import eig, eigh\n",
    "            eig_fn = eig if not self.is_undirected else eigh\n",
    "            eig_vals, eig_vecs = eig_fn(L.todense())\n",
    "        else:\n",
    "            from scipy.sparse.linalg import eigs, eigsh\n",
    "            eig_fn = eigs if not self.is_undirected else eigsh\n",
    "            eig_vals, eig_vecs = eig_fn(\n",
    "                L,\n",
    "                k=self.k + 1,\n",
    "                which='SR' if not self.is_undirected else 'SA',\n",
    "                return_eigenvectors=True,\n",
    "                **self.kwargs,\n",
    "            )\n",
    "    \n",
    "        eig_vecs = np.real(eig_vecs[:, eig_vals.argsort()])\n",
    "        pe = torch.from_numpy(eig_vecs[:, 1:self.k + 1])\n",
    "    \n",
    "        # l2 normalization\n",
    "        pe = torch.nn.functional.normalize(pe, p=2, dim=1)\n",
    "\n",
    "        # Switch signs for more robust generalization\n",
    "        sign = -1 + 2 * torch.randint(0, 2, (self.k, ))\n",
    "        pe *= sign\n",
    "        \n",
    "        # Concatenate \n",
    "        data.x = torch.cat([data.x, pe], dim=-1)\n",
    "    \n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import LRGBDataset\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GCN\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from torch_geometric.data import Data\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# Define the Laplacian PE transform\n",
    "num_eigenvectors = 3\n",
    "transform = AddCustomLaplacianEigenvectorPE(k=num_eigenvectors, attr_name=None)\n",
    "\n",
    "# Load Dataset\n",
    "dataset = LRGBDataset(root='data/LRGBDataset', name='Peptides-struct', pre_transform=transform)\n",
    "train_dataset = LRGBDataset(root='data/LRGBDataset', name='Peptides-struct', split='train', pre_transform=transform)\n",
    "val_dataset = LRGBDataset(root='data/LRGBDataset', name='Peptides-struct', split='val', pre_transform=transform)\n",
    "test_dataset = LRGBDataset(root='data/LRGBDataset', name='Peptides-struct', split='test', pre_transform=transform)\n",
    "\n",
    "\n",
    "# mlp function from source code\n",
    "# define MLP head as defined in the second paepr of LRGB\n",
    "class MLPGraphHead(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    MLP prediction head for graph prediction tasks.\n",
    "\n",
    "    Args:\n",
    "        hidden_channels (int): Input dimension.\n",
    "        out_channels (int): Output dimension. For binary prediction, dim_out=1.\n",
    "        L (int): Number of hidden layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pooling_fun = global_mean_pool\n",
    "        dropout = 0.1\n",
    "        L = 3\n",
    "\n",
    "        layers = []\n",
    "        for _ in range(L - 1):\n",
    "            layers.append(torch.nn.Dropout(dropout))\n",
    "            layers.append(torch.nn.Linear(hidden_channels, hidden_channels, bias=True))\n",
    "            layers.append(torch.nn.GELU())  # GELU before\n",
    "\n",
    "        # layers.append(torch.nn.BatchNorm1d(hidden_channels, track_running_stats=False))\n",
    "        layers.append(torch.nn.Dropout(dropout))\n",
    "        layers.append(torch.nn.Linear(hidden_channels, out_channels, bias=True))\n",
    "        self.mlp = torch.nn.Sequential(*layers)\n",
    "\n",
    "    # def _scale_and_shift(self, x):\n",
    "    # return x\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = self.pooling_fun(batch.x, batch.batch)\n",
    "        return self.mlp(x)\n",
    "\n",
    "# final function adding mlp head from source code\n",
    "class newGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n",
    "        super(newGCN, self).__init__()\n",
    "\n",
    "        # Define GCN layers with edge attributes\n",
    "        self.gcn = GCN(\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels=hidden_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            num_layers=num_layers,\n",
    "            act='gelu',\n",
    "            dropout=0.1,\n",
    "            norm='batch',\n",
    "            norm_kwargs={'track_running_stats': False}\n",
    "        )\n",
    "\n",
    "        # Replace the prediction head with MLPGraphHead\n",
    "        self.head = MLPGraphHead(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, batch=None):\n",
    "        # Apply GCN layers with edge attributes\n",
    "        x = self.gcn(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "        # Create a batch object for MLPGraphHead\n",
    "        batch_data = BatchData(x, batch)\n",
    "        # Pass through MLPGraphHead\n",
    "        return self.head(batch_data)\n",
    "\n",
    "\n",
    "\n",
    "class BatchData:\n",
    "    def __init__(self, x, batch):\n",
    "        self.x = x\n",
    "        self.batch = batch\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the GCN Model\n",
    "model = newGCN(\n",
    "    in_channels=dataset[0].num_node_features,\n",
    "    hidden_channels=235,\n",
    "    num_layers=6,\n",
    "    out_channels=11  # Number of regression tasks\n",
    ").to(device)\n",
    "print(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Learning rate scheduler\n",
    "# Define the warmup and cosine decay schedule\n",
    "def cosine_with_warmup(epoch):\n",
    "    if epoch < warmup_epochs:\n",
    "        # Linear warmup\n",
    "        return epoch / warmup_epochs\n",
    "    else:\n",
    "        # Cosine decay\n",
    "        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
    "        return 0.5 * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "\n",
    "# Parameters\n",
    "warmup_epochs = 5\n",
    "total_epochs = 250  # Total training epochs\n",
    "\n",
    "# Define the scheduler\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=cosine_with_warmup)\n",
    "\n",
    "torch.manual_seed(3)\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the training loop\n",
    "criterion = torch.nn.L1Loss()  # For MAE-based regression\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        # data = compute_laplacian_pe(data)\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        data.x = data.x.float()\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_attr=data.edge_attr, batch=data.batch)\n",
    "        loss = criterion(out, data.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "# Evaluation Function\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0  # Track loss for the scheduler\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            # data = compute_laplacian_pe(data)\n",
    "            data = data.to(device)\n",
    "            data.x = data.x.float()\n",
    "            out = model(x=data.x, edge_index=data.edge_index, edge_attr=data.edge_attr, batch=data.batch)\n",
    "            loss = criterion(out, data.y.float())  # Compute loss\n",
    "            total_loss += loss.item()\n",
    "            pred = out.cpu().numpy()\n",
    "            labels = data.y.cpu().numpy()  # Squeeze to remove single-dimensional entries\n",
    "            all_preds.append(pred)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    mae = mean_absolute_error(all_labels, all_preds)\n",
    "    r2 = r2_score(all_labels, all_preds, multioutput='uniform_average')  # Average across all tasks\n",
    "    return mae, r2, total_loss / len(loader)\n",
    "\n",
    "\n",
    "# Training\n",
    "# In the main training loop\n",
    "# Capture metrics during training\n",
    "\n",
    "\n",
    "# Create figure and subplots before the training loop\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# SOTA GCN baseline value\n",
    "sota_gcn_value = 0.2460\n",
    "\n",
    "# Initialize empty lists for metrics\n",
    "epochs = []\n",
    "train_losses = []\n",
    "train_maes = []\n",
    "train_r2s = []\n",
    "test_maes = []\n",
    "test_r2s = []\n",
    "val_maes = []\n",
    "val_r2s = []\n",
    "\n",
    "# Training loop with live plotting\n",
    "for epoch in range(1, (total_epochs + 1)):\n",
    "    # Clear the previous plots\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Perform training and evaluation\n",
    "    loss = train()\n",
    "    # Perform validation and testing\n",
    "    val_mae, val_r2, val_loss = test(val_loader)\n",
    "    test_mae, test_r2, _ = test(test_loader)\n",
    "    train_mae, train_r2, _ = test(train_loader)\n",
    "\n",
    "    # Step the scheduler\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print epoch information\n",
    "    print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, Val MAE: {val_mae:.4f}, Val R2: {val_r2:.4f}, '\n",
    "          f'Test MAE: {test_mae:.4f}, Test R2: {test_r2:.4f}')\n",
    "\n",
    "    # Store metrics\n",
    "    epochs.append(epoch)\n",
    "    train_losses.append(loss)\n",
    "    train_maes.append(train_mae)\n",
    "    train_r2s.append(train_r2)\n",
    "    test_maes.append(test_mae)\n",
    "    test_r2s.append(test_r2)\n",
    "    val_maes.append(val_mae)\n",
    "    val_r2s.append(val_r2)\n",
    "\n",
    "    # Create subplots\n",
    "    plt.clf()  # Clear the entire current figure\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Plot 1: Training Loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, train_losses, color='blue', label='Train Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot 2: MAE Comparison\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, train_maes, color='green', label='Train MAE')\n",
    "    plt.plot(epochs, val_maes, color='yellow', label='Val MAE')\n",
    "    plt.plot(epochs, test_maes, color='red', label='Test MAE')\n",
    "    plt.axhline(y=sota_gcn_value, color='black', linestyle='--', label='SOTA TEST GCN')\n",
    "    plt.title('Mean Absolute Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot 3: R2 Score Comparison\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs, train_r2s, color='purple', label='Train R2')\n",
    "    plt.plot(epochs, val_r2s, color='cyan', label='Val R2')\n",
    "    plt.plot(epochs, test_r2s, color='orange', label='Test R2')\n",
    "    plt.title('R2 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('R2 Score')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot 4: Combined Metrics Normalized\n",
    "    plt.subplot(2, 2, 4)\n",
    "    # Normalize metrics to 0-1 range for comparison\n",
    "    train_losses_norm = (np.array(train_losses) - np.min(train_losses)) / (np.max(train_losses) - np.min(train_losses))\n",
    "    train_maes_norm = (np.array(train_maes) - np.min(train_maes)) / (np.max(train_maes) - np.min(train_maes))\n",
    "    train_r2s_norm = (np.array(train_r2s) - np.min(train_r2s)) / (np.max(train_r2s) - np.min(train_r2s))\n",
    "\n",
    "    plt.plot(epochs, train_losses_norm, color='blue', label='Normalized Train Loss')\n",
    "    plt.plot(epochs, train_maes_norm, color='green', label='Normalized Train MAE')\n",
    "    plt.plot(epochs, train_r2s_norm, color='purple', label='Normalized Train R2')\n",
    "    plt.title('Normalized Metrics')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Normalized Value')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.pause(0.1)  # Small pause to update the plot\n",
    "\n",
    "# Final plot after training completes\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphgps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
