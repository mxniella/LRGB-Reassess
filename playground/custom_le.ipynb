{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch-geometric\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import LRGBDataset\n",
    "import torch_geometric.typing\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "from torch_geometric.utils import (\n",
    "    get_laplacian,\n",
    "    get_self_loop_attr,\n",
    "    is_torch_sparse_tensor,\n",
    "    scatter,\n",
    "    to_edge_index,\n",
    "    to_scipy_sparse_matrix,\n",
    "    to_torch_coo_tensor,\n",
    "    to_torch_csr_tensor,\n",
    ")\n",
    "from typing import Any, Optional\n",
    "import numpy as np\n",
    "from torch_geometric.nn import global_mean_pool, GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(features):\n",
    "    return (features - features.mean(dim=0)) / (features.std(dim=0) + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddCustomLaplacianEigenvectorPE(BaseTransform):\n",
    "    r\"\"\"Adds the Laplacian eigenvector positional encoding from the\n",
    "    `\"Benchmarking Graph Neural Networks\" <https://arxiv.org/abs/2003.00982>`_\n",
    "    paper to the given graph\n",
    "    (functional name: :obj:`add_laplacian_eigenvector_pe`).\n",
    "\n",
    "    Args:\n",
    "        k (int): The number of non-trivial eigenvectors to consider.\n",
    "        attr_name (str, optional): The attribute name of the data object to add\n",
    "            positional encodings to. If set to :obj:`None`, will be\n",
    "            concatenated to :obj:`data.x`.\n",
    "            (default: :obj:`\"laplacian_eigenvector_pe\"`)\n",
    "        is_undirected (bool, optional): If set to :obj:`True`, this transform\n",
    "            expects undirected graphs as input, and can hence speed up the\n",
    "            computation of eigenvectors. (default: :obj:`False`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :meth:`scipy.sparse.linalg.eigs` (when :attr:`is_undirected` is\n",
    "            :obj:`False`) or :meth:`scipy.sparse.linalg.eigsh` (when\n",
    "            :attr:`is_undirected` is :obj:`True`).\n",
    "    \"\"\"\n",
    "    # Number of nodes from which to use sparse eigenvector computation:\n",
    "    SPARSE_THRESHOLD: int = 100\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        k: int,\n",
    "        attr_name: Optional[str] = 'laplacian_eigenvector_pe',\n",
    "        is_undirected: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        self.k = k\n",
    "        self.attr_name = attr_name\n",
    "        self.is_undirected = is_undirected\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def forward(self, data: Data) -> Data:\n",
    "        assert data.edge_index is not None\n",
    "        num_nodes = data.num_nodes\n",
    "        assert num_nodes is not None\n",
    "    \n",
    "        edge_index, edge_weight = get_laplacian(\n",
    "            data.edge_index,\n",
    "            data.edge_weight,\n",
    "            normalization=None,  # Skip Laplacian normalization\n",
    "            num_nodes=num_nodes,\n",
    "        )\n",
    "    \n",
    "        L = to_scipy_sparse_matrix(edge_index, edge_weight, num_nodes)\n",
    "    \n",
    "        if num_nodes < self.SPARSE_THRESHOLD:\n",
    "            from numpy.linalg import eig, eigh\n",
    "            eig_fn = eig if not self.is_undirected else eigh\n",
    "            eig_vals, eig_vecs = eig_fn(L.todense())\n",
    "        else:\n",
    "            from scipy.sparse.linalg import eigs, eigsh\n",
    "            eig_fn = eigs if not self.is_undirected else eigsh\n",
    "            eig_vals, eig_vecs = eig_fn(\n",
    "                L,\n",
    "                k=self.k + 1,\n",
    "                which='SR' if not self.is_undirected else 'SA',\n",
    "                return_eigenvectors=True,\n",
    "                **self.kwargs,\n",
    "            )\n",
    "    \n",
    "        eig_vecs = np.real(eig_vecs[:, eig_vals.argsort()])\n",
    "        pe = torch.from_numpy(eig_vecs[:, 1:self.k + 1])\n",
    "    \n",
    "        # Standardize features and Laplacian encodings\n",
    "        data.x = normalize_features(data.x)\n",
    "        pe = normalize_features(pe)\n",
    "        \n",
    "        # Concatenate standardized features\n",
    "        data.x = torch.cat([data.x, pe], dim=-1)\n",
    "    \n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class newGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers, out_channels):\n",
    "        # Initialize Superclass\n",
    "        super().__init__()\n",
    "\n",
    "        # GCN layers\n",
    "        self.gcn = GCN(in_channels=in_channels, \n",
    "                       hidden_channels=hidden_channels, \n",
    "                       out_channels=hidden_channels,\n",
    "                       num_layers=6, \n",
    "                       act='relu', \n",
    "                       dropout=0.1, \n",
    "                       norm='batch', \n",
    "                       norm_kwargs={'track_running_stats': False})\n",
    "\n",
    "        self.linear = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "        # Multi-layer prediction head\n",
    "        self.prediction_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels),\n",
    "            torch.nn.BatchNorm1d(hidden_channels, track_running_stats=False),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels),\n",
    "            # torch.nn.BatchNorm1d(hidden_channels, track_running_stats=False),\n",
    "            torch.nn.ReLU(),\n",
    "            # torch.nn.Dropout(0.2),\n",
    "\n",
    "            torch.nn.Linear(hidden_channels, out_channels)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, batch=None):\n",
    "        x = x.float()\n",
    "        \n",
    "        # pass trough GCN\n",
    "        x = self.gcn(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        \n",
    "        # perform global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # pass through prediciton head\n",
    "        x = self.prediction_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import LRGBDataset\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "num_eigenvectors = 4\n",
    "transform = AddCustomLaplacianEigenvectorPE(k=num_eigenvectors, attr_name=None)\n",
    "\n",
    "# Split datasets\n",
    "train_dataset = LRGBDataset(root='data/LRGBDataset', name='Peptides-struct', split='train', pre_transform=transform)\n",
    "val_dataset = LRGBDataset(root='data/LRGBDataset', name='Peptides-struct', split='val', pre_transform=transform)\n",
    "test_dataset = LRGBDataset(root='data/LRGBDataset', name='Peptides-struct', split='test', pre_transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "in_channels = train_dataset[0].x.shape[1]\n",
    "# Initialize the GCN Model\n",
    "model = newGCN(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=235,\n",
    "    num_layers=6,\n",
    "    out_channels=11  # Number of regression tasks\n",
    ").to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=20,\n",
    "    min_lr=1e-5\n",
    ")\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "# Define the training loop\n",
    "torch.manual_seed(3)\n",
    "criterion = torch.nn.L1Loss()  # For MAE-based regression\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        data.x = data.x.float()\n",
    "\n",
    "        out = model(x=data.x, edge_index=data.edge_index, edge_attr=data.edge_attr, batch=data.batch)\n",
    "        loss = criterion(out, data.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "# Evaluation Function\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            # data = compute_laplacian_pe(data)\n",
    "            data = data.to(device)\n",
    "            data.x = data.x.float()\n",
    "            out = model(x=data.x, edge_index=data.edge_index, edge_attr=data.edge_attr, batch=data.batch)\n",
    "            pred = out.cpu().numpy()\n",
    "            labels = data.y.cpu().numpy()  # Squeeze to remove single-dimensional entries\n",
    "            all_preds.append(pred)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    mae = mean_absolute_error(all_labels, all_preds)\n",
    "    r2 = r2_score(all_labels, all_preds, multioutput='uniform_average')  # Average across all tasks\n",
    "    return mae, r2\n",
    "\n",
    "\n",
    "# Training\n",
    "# In the main training loop\n",
    "# Capture metrics during training\n",
    "\n",
    "\n",
    "# Create figure and subplots before the training loop\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# SOTA GCN baseline value\n",
    "sota_gcn_value = 0.2460\n",
    "\n",
    "# Initialize empty lists for metrics\n",
    "epochs = []\n",
    "train_losses = []\n",
    "train_maes = []\n",
    "train_r2s = []\n",
    "test_maes = []\n",
    "test_r2s = []\n",
    "val_maes = []\n",
    "val_r2s = []\n",
    "\n",
    "# Training loop with live plotting\n",
    "for epoch in range(1, 251):\n",
    "    # Clear the previous plots\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Perform training and evaluation\n",
    "    loss = train()\n",
    "    val_mae, val_r2 = test(val_loader)\n",
    "    test_mae, test_r2 = test(test_loader)\n",
    "    train_mae, train_r2 = test(train_loader)\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_mae)\n",
    "\n",
    "    # Print epoch information\n",
    "    print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, Val MAE: {val_mae:.4f}, Val R2: {val_r2:.4f}, '\n",
    "          f'Test MAE: {test_mae:.4f}, Test R2: {test_r2:.4f}')\n",
    "\n",
    "    # Store metrics\n",
    "    epochs.append(epoch)\n",
    "    train_losses.append(loss)\n",
    "    train_maes.append(train_mae)\n",
    "    train_r2s.append(train_r2)\n",
    "    test_maes.append(test_mae)\n",
    "    test_r2s.append(test_r2)\n",
    "    val_maes.append(val_mae)\n",
    "    val_r2s.append(val_r2)\n",
    "\n",
    "    # Create subplots\n",
    "    plt.clf()  # Clear the entire current figure\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Plot 1: Training Loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, train_losses, color='blue', label='Train Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot 2: MAE Comparison\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, train_maes, color='green', label='Train MAE')\n",
    "    plt.plot(epochs, val_maes, color='yellow', label='Val MAE')\n",
    "    plt.plot(epochs, test_maes, color='red', label='Test MAE')\n",
    "    plt.axhline(y=sota_gcn_value, color='black', linestyle='--', label='SOTA TEST GCN')\n",
    "    plt.title('Mean Absolute Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot 3: R2 Score Comparison\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs, train_r2s, color='purple', label='Train R2')\n",
    "    plt.plot(epochs, val_r2s, color='cyan', label='Val R2')\n",
    "    plt.plot(epochs, test_r2s, color='orange', label='Test R2')\n",
    "    plt.title('R2 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('R2 Score')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot 4: Combined Metrics Normalized\n",
    "    plt.subplot(2, 2, 4)\n",
    "    # Normalize metrics to 0-1 range for comparison\n",
    "    train_losses_norm = (np.array(train_losses) - np.min(train_losses)) / (np.max(train_losses) - np.min(train_losses))\n",
    "    train_maes_norm = (np.array(train_maes) - np.min(train_maes)) / (np.max(train_maes) - np.min(train_maes))\n",
    "    train_r2s_norm = (np.array(train_r2s) - np.min(train_r2s)) / (np.max(train_r2s) - np.min(train_r2s))\n",
    "\n",
    "    plt.plot(epochs, train_losses_norm, color='blue', label='Normalized Train Loss')\n",
    "    plt.plot(epochs, train_maes_norm, color='green', label='Normalized Train MAE')\n",
    "    plt.plot(epochs, train_r2s_norm, color='purple', label='Normalized Train R2')\n",
    "    plt.title('Normalized Metrics')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Normalized Value')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.pause(0.1)  # Small pause to update the plot\n",
    "\n",
    "# Final plot after training completes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphgps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
