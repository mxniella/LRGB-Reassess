{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.datasets import LRGBDataset\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "from torch_geometric.utils import get_laplacian, to_scipy_sparse_matrix\n",
    "\n",
    "\n",
    "class LaplacianPEWithAtomEncoding(BaseTransform):\n",
    "    def __init__(self, num_eigenvectors=10, emb_dim=128):\n",
    "        self.num_eigenvectors = num_eigenvectors\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # Compute Laplacian eigenvalues and eigenvectors\n",
    "        laplacian_norm_type = \"sym\"\n",
    "        L = to_scipy_sparse_matrix(\n",
    "            *get_laplacian(data.edge_index, normalization=laplacian_norm_type, num_nodes=data.num_nodes)\n",
    "        )\n",
    "        evals, evects = np.linalg.eigh(L.toarray())\n",
    "        \n",
    "        # Select top-k eigenvectors and normalize\n",
    "        max_freqs = self.num_eigenvectors\n",
    "        idx = evals.argsort()[:self.num_eigenvectors]\n",
    "        evals, evects = evals[idx], np.real(evects[:, idx])\n",
    "        evals = torch.from_numpy(np.real(evals)).clamp_min(0)\n",
    "\n",
    "        evects = torch.from_numpy(evects).float()\n",
    "        evects = self._l2_normalize(evects)\n",
    "\n",
    "        # Pad eigenvalues and eigenvectors if necessary\n",
    "        num_nodes = data.num_nodes\n",
    "        if num_nodes < max_freqs:\n",
    "            evects = F.pad(evects, (0, max_freqs - num_nodes), value=0.0)  # Use 0.0 instead of NaN\n",
    "            evals = F.pad(evals, (0, max_freqs - num_nodes), value=0.0).unsqueeze(0)  # Use 0.0 instead of NaN\n",
    "        else:\n",
    "            evals = evals.unsqueeze(0)\n",
    "\n",
    "        evals = evals.repeat(num_nodes, 1).unsqueeze(2)\n",
    "\n",
    "        # Concatenate Laplacian PE with node features\n",
    "        data.x = torch.cat([data.x, evects], dim=1)\n",
    "        return data\n",
    "    \n",
    "    def _l2_normalize(self, eigenvectors, eps=1e-12):\n",
    "        denom = eigenvectors.norm(p=2, dim=0, keepdim=True).clamp_min(eps)\n",
    "        return eigenvectors / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Peptides-struct\"\n",
    "\n",
    "# Define the transform\n",
    "num_eigenvectors = 10\n",
    "emb_dim = 128\n",
    "transform = LaplacianPEWithAtomEncoding(num_eigenvectors=num_eigenvectors, emb_dim=emb_dim)\n",
    "\n",
    "# Load dataset with the pre-transform\n",
    "dataset = LRGBDataset(root=\"data/LRGBDataset\", name=name, pre_transform=transform)\n",
    "print(f\"Dataset loaded with {len(dataset)} graphs.\")\n",
    "\n",
    "# Save the transformed dataset\n",
    "output_path = \"Peptides-struct-transformed.pt\"\n",
    "torch.save(dataset, output_path)\n",
    "print(f\"Transformed dataset saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphgps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
